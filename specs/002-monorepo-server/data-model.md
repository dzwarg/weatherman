# Data Model: Monorepo Server Component

**Feature**: 002-monorepo-server \
**Created**: 2025-12-19

## Overview

This document defines the data structures exchanged between the frontend and server components, and the entities used within the server for processing. The server acts as a proxy for weather data and provides dynamic clothing recommendations based on user profiles.

## Key Entities

### 1. Weather Request (Frontend → Server)

**Purpose**: Request weather data for a specific location

**Structure**:
```javascript
{
  lat: number,      // Latitude (-90 to 90)
  lon: number,      // Longitude (-180 to 180)
  units?: string    // Optional: 'imperial' (default) or 'metric'
}
```

**Validation Rules**:
- `lat` is required, must be between -90 and 90
- `lon` is required, must be between -180 and 180
- `units` defaults to 'imperial' if not provided

**Example**:
```javascript
{
  lat: 42.3601,
  lon: -71.0589,
  units: 'imperial'
}
```

---

### 2. Weather Response (Server → Frontend)

**Purpose**: Weather data returned from server proxy

**Structure**: Maintains existing `WeatherData` model format from frontend

```javascript
{
  location: {
    name: string,
    lat: number,
    lon: number,
    timezone: string
  },
  current: {
    temperature: number,        // °F or °C
    feelsLike: number,
    conditions: string,         // e.g., "Clear", "Rain", "Snow"
    precipitationProbability: number,  // 0-100
    windSpeed: number,          // mph or km/h
    humidity: number,           // 0-100
    uvIndex: number,
    timestamp: string           // ISO 8601
  },
  hourlyForecast: [
    {
      timestamp: string,
      temperature: number,
      precipitationProbability: number,
      conditions: string
    }
  ],
  dailyForecast: [
    {
      date: string,
      temperatureHigh: number,
      temperatureLow: number,
      precipitationProbability: number,
      conditions: string
    }
  ],
  alerts: [
    {
      title: string,
      description: string,
      severity: string,
      start: string,
      end: string
    }
  ],
  fetchedAt: string,          // ISO 8601 timestamp
  cacheExpiry: string         // ISO 8601 timestamp
}
```

**Validation Rules**:
- Temperature must be between -100°F and 150°F
- Precipitation probability must be 0-100
- Wind speed must be >= 0
- Humidity must be 0-100
- UV index must be >= 0
- All timestamps must be valid ISO 8601 format

---

### 3. Recommendation Request (Frontend → Server)

**Purpose**: Request clothing recommendations based on profile, weather, and voice prompt context

**Structure**:
```javascript
{
  profile: {
    id: string,           // e.g., "4yo-girl", "7yo-boy", "10yo-boy"
    age: number,          // 4, 7, or 10
    gender: string        // "girl" or "boy"
  },
  weather: {
    temperature: number,
    feelsLike: number,
    conditions: string,
    precipitationProbability: number,
    windSpeed: number,
    uvIndex: number
  },
  prompt?: string,        // Optional: User's voice input for context
  timeframe?: string      // Optional: "morning", "afternoon", "evening", "today"
}
```

**Validation Rules**:
- `profile` is required
  - `profile.id` must be one of: "4yo-girl", "7yo-boy", "10yo-boy"
  - `profile.age` must be 4, 7, or 10
  - `profile.gender` must be "girl" or "boy"
- `weather` is required
  - `temperature` is required (between -100 and 150)
  - `conditions` is required (non-empty string)
  - Other weather fields optional but validated if present
- `prompt` is optional
  - String, no length limit (reasonable max ~500 chars)
  - Will be analyzed for context keywords (playground, party, school, etc.)
  - Empty string treated same as undefined (no context)
- `timeframe` is optional, defaults to "today"

**Example**:
```javascript
{
  profile: {
    id: "4yo-girl",
    age: 4,
    gender: "girl"
  },
  weather: {
    temperature: 35,
    feelsLike: 28,
    conditions: "Rain",
    precipitationProbability: 80,
    windSpeed: 12,
    uvIndex: 2
  },
  prompt: "What should I wear to the playground today?",
  timeframe: "morning"
}
```

---

### 4. Recommendation Response (Server → Frontend)

**Purpose**: Clothing recommendations with spoken response

**Structure**: Maintains existing `ClothingRecommendation` model format

```javascript
{
  id: string,               // UUID generated by server
  profileId: string,        // Matches request profile.id
  weatherData: {
    temperature: number,
    feelsLike: number,
    conditions: string,
    precipitationProbability: number,
    windSpeed: number,
    uvIndex: number
  },
  recommendations: {
    baseLayers: [
      {
        item: string,       // e.g., "Long-sleeve shirt"
        reason: string      // e.g., "For warmth"
      }
    ],
    outerwear: [
      {
        item: string,
        reason: string
      }
    ],
    bottoms: [
      {
        item: string,
        reason: string
      }
    ],
    accessories: [
      {
        item: string,
        reason: string
      }
    ],
    footwear: [
      {
        item: string,
        reason: string
      }
    ]
  },
  spokenResponse: string,   // Child-friendly natural language summary
  confidence: number,       // 0.0 to 1.0
  createdAt: string,        // ISO 8601 timestamp
  feedbackProvided: boolean // Always false from server
}
```

**Validation Rules**:
- `id` must be valid UUID
- `profileId` must match one of the valid profile IDs
- `weatherData` must contain valid temperature and conditions
- `recommendations` must have at least one category with items
- `spokenResponse` must be non-empty string
- `confidence` must be between 0.0 and 1.0
- `createdAt` must be valid ISO 8601 timestamp

**Example**:
```javascript
{
  id: "f47ac10b-58cc-4372-a567-0e02b2c3d479",
  profileId: "4yo-girl",
  weatherData: {
    temperature: 35,
    feelsLike: 28,
    conditions: "Rain",
    precipitationProbability: 80,
    windSpeed: 12,
    uvIndex: 2
  },
  recommendations: {
    baseLayers: [
      { item: "Long-sleeve t-shirt", reason: "To stay warm" }
    ],
    outerwear: [
      { item: "Warm winter coat", reason: "It's cold and rainy" },
      { item: "Raincoat or poncho", reason: "To stay dry" }
    ],
    bottoms: [
      { item: "Pull-on pants or leggings", reason: "Easy to put on and warm" }
    ],
    accessories: [
      { item: "Warm hat", reason: "To keep your head warm" },
      { item: "Gloves or mittens", reason: "To keep your hands warm" },
      { item: "Umbrella", reason: "To stay dry in the rain" }
    ],
    footwear: [
      { item: "Rain boots", reason: "To keep your feet dry" }
    ]
  },
  spokenResponse: "It's cold and rainy today! You should wear a long-sleeve shirt, warm coat, and rain boots. Don't forget your umbrella, hat, and mittens to stay warm and dry!",
  confidence: 0.95,
  createdAt: "2025-12-19T10:30:00.000Z",
  feedbackProvided: false
}
```

---

### 5. Error Response (Server → Frontend)

**Purpose**: Standardized error format for all API failures

**Structure**:
```javascript
{
  error: {
    code: string,         // Machine-readable error code
    message: string,      // Human-readable error message
    details?: object      // Optional additional error details
  },
  timestamp: string       // ISO 8601 timestamp
}
```

**Error Codes**:
- `INVALID_REQUEST`: Request validation failed
- `WEATHER_API_ERROR`: Weather API returned error
- `WEATHER_API_TIMEOUT`: Weather API timed out (5 seconds)
- `RATE_LIMIT_EXCEEDED`: Too many requests to weather API
- `INTERNAL_ERROR`: Unexpected server error
- `SERVICE_UNAVAILABLE`: Server dependency unavailable

**Example**:
```javascript
{
  error: {
    code: "WEATHER_API_TIMEOUT",
    message: "Weather service did not respond within 5 seconds",
    details: {
      timeout: 5000,
      url: "api.openweathermap.org"
    }
  },
  timestamp: "2025-12-19T10:30:00.000Z"
}
```

---

## Ollama Service Entities

### 8. Ollama Request (Server → Ollama)

**Purpose**: Request clothing recommendations from locally hosted Ollama LLM service

**Structure**:
```javascript
{
  model: string,              // e.g., "mistral:latest"
  prompt: string,             // Structured prompt with profile, weather, context
  stream: boolean,            // Always false for this use case
  options: {
    temperature: number,      // 0.7 for balanced creativity
    top_p: number,            // 0.9 for diverse outputs
    max_tokens: number        // 500 tokens max
  }
}
```

**Validation Rules**:
- `model` must be one of installed Ollama models
- `prompt` must be non-empty string
- `temperature` should be between 0.0 and 1.0
- `top_p` should be between 0.0 and 1.0
- `max_tokens` should be positive integer

**Example**:
```javascript
{
  model: "mistral:latest",
  prompt: "You are a helpful clothing advisor for children. Provide age-appropriate clothing recommendations.\n\nProfile: 4-year-old girl\nTemperature: 35°F (feels like 28°F)\nConditions: Rain\nPrecipitation: 80%\nWind: 12 mph\nActivity context: playground\n\nProvide structured recommendations with categories:\n- Base Layers\n- Outerwear\n- Bottoms\n- Accessories\n- Footwear\n\nFor each item, include a child-friendly reason (1 sentence).",
  stream: false,
  options: {
    temperature: 0.7,
    top_p: 0.9,
    max_tokens: 500
  }
}
```

---

### 9. Ollama Response (Ollama → Server)

**Purpose**: LLM-generated clothing recommendations in free-text format

**Structure**:
```javascript
{
  model: string,              // Model used for generation
  created_at: string,         // ISO 8601 timestamp
  response: string,           // Free-text recommendations (parsed by server)
  done: boolean,              // Always true for non-streaming
  context: number[],          // Model context (not used)
  total_duration: number,     // Total time in nanoseconds
  load_duration: number,      // Model load time in nanoseconds
  prompt_eval_count: number,  // Tokens in prompt
  prompt_eval_duration: number, // Prompt evaluation time
  eval_count: number,         // Tokens in response
  eval_duration: number       // Response generation time
}
```

**Response Parsing**:
Server must parse the free-text `response` field to extract:
- Category headings (Base Layers, Outerwear, etc.)
- Clothing items within each category
- Reasons for each item

**Example**:
```javascript
{
  model: "mistral:latest",
  created_at: "2025-12-19T10:30:00.000Z",
  response: "For a 4-year-old girl going to the playground in cold, rainy weather:\n\n**Base Layers:**\n- Long-sleeve t-shirt: To stay warm\n\n**Outerwear:**\n- Warm winter coat: It's cold and rainy\n- Raincoat or poncho: To stay dry\n\n**Bottoms:**\n- Pull-on pants or leggings: Easy to put on and warm\n\n**Accessories:**\n- Warm hat: To keep your head warm\n- Gloves or mittens: To keep your hands warm\n- Umbrella: To stay dry in the rain\n\n**Footwear:**\n- Rain boots: To keep your feet dry",
  done: true,
  context: [/* array of numbers */],
  total_duration: 1234567890,
  load_duration: 123456,
  prompt_eval_count: 45,
  prompt_eval_duration: 12345678,
  eval_count: 120,
  eval_duration: 987654321
}
```

**Parsing Strategy**:
1. Split response by category headings (lines starting with `**`)
2. Extract items and reasons (lines starting with `-`)
3. Handle variations in formatting (LLM may vary punctuation/structure)
4. Generate fallback response if parsing fails

---

### 10. Frontend Mock Response (Development Only)

**Purpose**: Pre-generated discrete responses for 3 profiles during frontend development

**Structure**: Matches Ollama response format exactly

**Mock Files**:
- `packages/frontend/src/mocks/ollama/4yo-girl-cold-rainy.json`
- `packages/frontend/src/mocks/ollama/7yo-boy-moderate.json`
- `packages/frontend/src/mocks/ollama/10yo-boy-hot-sunny.json`

**Example Mock** (`4yo-girl-cold-rainy.json`):
```javascript
{
  "model": "mistral:latest",
  "created_at": "2025-12-19T10:00:00.000Z",
  "response": "For a 4-year-old girl in cold, rainy weather:\n\n**Base Layers:**\n- Long-sleeve t-shirt: To stay warm\n\n**Outerwear:**\n- Warm winter coat: It's cold and rainy\n- Raincoat or poncho: To stay dry\n\n**Bottoms:**\n- Pull-on pants or leggings: Easy to put on and warm\n\n**Accessories:**\n- Warm hat: To keep your head warm\n- Mittens: To keep your hands warm\n- Umbrella: To stay dry in the rain\n\n**Footwear:**\n- Rain boots: To keep your feet dry",
  "done": true,
  "total_duration": 1000000000,
  "eval_count": 100
}
```

**Usage**:
Frontend service checks `VITE_USE_MOCK_OLLAMA` environment variable. If `true`, returns mock response based on profile ID and weather conditions instead of calling server API.

---

## Server Internal Entities

### 6. Rate Limit State (Server Internal)

**Purpose**: Track API rate limiting per time window

**Structure**:
```javascript
{
  apiKey: string,           // Which API (e.g., "weather-api")
  windowStart: number,      // Unix timestamp (ms)
  windowDuration: number,   // Window size in ms (e.g., 60000 for 1 min)
  requestCount: number,     // Requests in current window
  maxRequests: number       // Max allowed in window
}
```

**Behavior**:
- In-memory storage (resets on server restart)
- Sliding window algorithm
- Returns 429 status when limit exceeded

---

### 7. Weather API Configuration (Server Internal)

**Purpose**: Configuration for external weather API

**Structure**:
```javascript
{
  apiKey: string,           // From environment variable
  baseUrl: string,          // API base URL
  timeout: number,          // Request timeout (5000ms)
  endpoints: {
    current: string,
    forecast: string,
    alerts: string
  },
  rateLimit: {
    maxRequests: number,    // e.g., 60
    windowMs: number        // e.g., 60000 (1 minute)
  }
}
```

---

## Data Flow

### Weather Proxy Flow

```
Frontend Request
  ↓
Server Validation
  ↓
Rate Limit Check
  ↓
Weather API Call (with timeout)
  ↓
Transform Response
  ↓
Return to Frontend
  ↓
Frontend Cache (Service Worker)
```

### Recommendation Flow (with Ollama)

```
Frontend Request (Profile + Weather + Prompt)
  ↓
Server Validation
  ↓
Build Structured Prompt
  ↓
Ollama API Call (10s timeout)
  ↓
Parse LLM Response
  ↓
Transform to Structured Format
  ↓
Generate Spoken Response
  ↓
Return to Frontend
  ↓
Frontend Cache (Service Worker)
```

**Fallback Flow (Ollama unavailable)**:
```
Ollama API Call → Connection Refused
  ↓
Load Rule-Based Fallback Logic
  ↓
Apply Profile Logic (age-appropriate, gender-specific)
  ↓
Generate Recommendations (hardcoded rules)
  ↓
Return to Frontend with Warning
```

**Frontend Mock Flow (Development)**:
```
Frontend Request (Profile + Weather)
  ↓
Check VITE_USE_MOCK_OLLAMA
  ↓ (true)
Select Mock Based on Profile + Weather
  ↓
Return Mock Ollama Response
  ↓
Parse and Display (bypasses server)
```

---

## Backward Compatibility

The server responses maintain 100% compatibility with existing frontend data models:
- `WeatherData` structure unchanged
- `ClothingRecommendation` structure unchanged
- Frontend models continue to work without modification
- Only the data **source** changes (server instead of direct API/hardcoded logic)

---

## Cache Strategy

**Frontend Caching** (Service Worker):
- Weather responses: 1 hour (existing strategy)
- Recommendation responses: 30 minutes (new)
- Cache key includes profile ID and weather snapshot

**Server Caching**:
- None initially (stateless server)
- Optional future enhancement: Redis cache for weather data

---

## Security Considerations

**API Keys**:
- Never included in responses to frontend
- Stored in server environment variables only
- Validated on server startup

**Request Validation**:
- All numeric fields validated for range
- Profile IDs must match predefined list
- No arbitrary string inputs accepted for profile data

**Rate Limiting**:
- Prevents frontend abuse of weather API
- Separate limits per endpoint if needed
- Returns clear error messages when exceeded
